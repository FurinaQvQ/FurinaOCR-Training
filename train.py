"""
åŸç¥OCRè®­ç»ƒè„šæœ¬ - ä¼˜åŒ–ç‰ˆ
é›†æˆæ··åˆç²¾åº¦è®­ç»ƒã€æ€§èƒ½ä¼˜åŒ–å™¨å’Œé«˜çº§ç›‘æ§åŠŸèƒ½
"""
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from PIL import ImageFont
from typing import Tuple
import datetime
import gc

from mona.datagen.datagen import DataGen
from mona.config import config, get_config_manager
from mona.nn import predict as predict_net
from mona.nn.model2 import Model2
from mona.text import get_lexicon
from mona.utils import logger

# å¯¼å…¥æ€§èƒ½ä¼˜åŒ–å™¨
from mona.training.performance_optimizer import PerformanceOptimizer, TrainingMonitor

# è®¾å¤‡æ£€æµ‹
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

# è·å–é…ç½®ç®¡ç†å™¨
config_manager = get_config_manager()

# åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
performance_optimizer = PerformanceOptimizer(config_manager.hardware)
logger.info("âœ… æ€§èƒ½ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")

# åˆå§‹åŒ–
lexicon = get_lexicon()
fonts = [ImageFont.truetype("./assets/genshin.ttf", i) for i in range(15, 90)]
datagen = DataGen(config, fonts, lexicon)

logger.info(f"è¯æ±‡è¡¨å¤§å°: {lexicon.lexicon_size()}")

# æ˜¾ç¤ºä¼˜åŒ–çŠ¶æ€
if config_manager.hardware["mixed_precision"]:
    logger.info("ğŸš€ æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
if config_manager.hardware["compile_model"]:
    logger.info("ğŸ”¥ æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å·²å¯ç”¨")


class AddGaussianNoise(nn.Module):
    """é«˜æ–¯å™ªå£°æ•°æ®å¢å¼º"""
    def __init__(self, mean: float = 0.0, std: float = 1/255):
        super().__init__()
        self.std = std
        self.mean = mean

    def forward(self, tensor: torch.Tensor) -> torch.Tensor:
        if self.training:
            return tensor + torch.randn(tensor.size(), device=tensor.device) * self.std + self.mean
        return tensor


class OnlineDataSet(Dataset):
    """åœ¨çº¿æ•°æ®é›†"""
    def __init__(self, size: int):
        self.size = size

    def __len__(self):
        return self.size

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, str]:
        im, text = datagen.generate_image()
        tensor = transforms.ToTensor()(im)
        return tensor, text


def get_target(s: list, lexicon) -> Tuple[torch.Tensor, torch.Tensor]:
    """è½¬æ¢ç›®æ ‡å­—ç¬¦ä¸²ä¸ºå¼ é‡"""
    target_length = []
    target_vector = []
    
    for target in s:
        target_length.append(len(target))
        for char in target:
            index = lexicon.word_to_index.get(char, 0)
            target_vector.append(index)

    return torch.LongTensor(target_vector), torch.LongTensor(target_length)


def validate_model(net: nn.Module, validate_loader: DataLoader, lexicon, device: str) -> float:
    """æ¨¡å‹éªŒè¯"""
    net.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for x, label in validate_loader:
            x = x.to(device)
            
            # ä½¿ç”¨æ··åˆç²¾åº¦è¿›è¡Œæ¨ç†
            with performance_optimizer.autocast_context():
                predict = predict_net(net, x, lexicon)
            
            correct += sum([1 if predict[i] == label[i] else 0 for i in range(len(label))])
            total += len(label)

    accuracy = correct / total if total > 0 else 0.0
    net.train()
    return accuracy


def cleanup_resources():
    """æ¸…ç†èµ„æº"""
    performance_optimizer.cleanup_memory(aggressive=True)
    logger.info("ğŸ’¾ å†…å­˜æ¸…ç†å®Œæˆ")


def train():
    """ä¼˜åŒ–çš„è®­ç»ƒå‡½æ•°"""
    logger.info("å¼€å§‹ä¼˜åŒ–è®­ç»ƒ...")
    
    # æ¨¡å‹åˆå§‹åŒ–
    net = Model2(lexicon.lexicon_size(), 1).to(device)
    logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in net.parameters()):,}")
    
    # åº”ç”¨æ€§èƒ½ä¼˜åŒ–
    net = performance_optimizer.optimize_model(net)
    
    # é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
    if config.get("pretrain", False):
        try:
            checkpoint = torch.load(f"models/{config['pretrain_name']}", 
                                  map_location=device, weights_only=True)
            net.load_state_dict(checkpoint)
            logger.info(f"âœ… åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {config['pretrain_name']}")
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¤±è´¥: {e}")

    # æ•°æ®å¢å¼º
    data_aug_transform = transforms.Compose([
        transforms.RandomApply([
            transforms.RandomChoice([
                transforms.GaussianBlur(1, 1),
                transforms.GaussianBlur(3, 3),
                transforms.GaussianBlur(5, 5),
            ])], p=0.5),
        transforms.RandomApply([
            transforms.RandomCrop(size=(31, 383)),
            transforms.Resize((32, 384), antialias=True),
        ], p=0.5),
        AddGaussianNoise(mean=0, std=1/255),
    ])
    
    # æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ - åº”ç”¨æ€§èƒ½ä¼˜åŒ–
    train_dataset = OnlineDataSet(config['train_size'])
    validate_dataset = OnlineDataSet(config['validate_size'])

    # ä¼˜åŒ–æ•°æ®åŠ è½½å™¨è®¾ç½®
    train_loader_kwargs = {
        "dataset": train_dataset,
        "shuffle": True,
        "batch_size": config["batch_size"],
    }
    train_loader_kwargs = performance_optimizer.optimize_dataloader(train_loader_kwargs)
    train_loader = DataLoader(**train_loader_kwargs)
    
    validate_loader_kwargs = {
        "dataset": validate_dataset,
        "batch_size": config["batch_size"],
    }
    validate_loader_kwargs = performance_optimizer.optimize_dataloader(validate_loader_kwargs)
    validate_loader = DataLoader(**validate_loader_kwargs)

    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adadelta(net.parameters())
    ctc_loss = nn.CTCLoss(blank=0, reduction="mean", zero_infinity=True).to(device)
    
    # åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨
    training_monitor = TrainingMonitor(log_interval=config["print_per"])
    
    # è®­ç»ƒå¾ªç¯
    total_epochs = config["epoch"]
    batch_count = 1
    best_accuracy = 0.0
    start_time = datetime.datetime.now()
    
    logger.info(f"ğŸš€ è®­ç»ƒè®¾ç½®: {total_epochs} epochs, batch_size={config['batch_size']}")
    
    # æ˜¾ç¤ºæ€§èƒ½ä¼˜åŒ–å»ºè®®
    suggestions = performance_optimizer.suggest_optimizations()
    for suggestion in suggestions:
        logger.info(f"ğŸ’¡ {suggestion}")
    
    for epoch in range(1, total_epochs + 1):
        for batch_idx, (x, label) in enumerate(train_loader, 1):
            step_start_time = datetime.datetime.now().timestamp()
            
            try:
                optimizer.zero_grad()
                target_vector, target_lengths = get_target(label, lexicon)
                target_vector, target_lengths = target_vector.to(device), target_lengths.to(device)
                x = x.to(device)

                # æ•°æ®å¢å¼º
                x = data_aug_transform(x)
                batch_size = x.size(0)

                # å‰å‘ä¼ æ’­ - ä½¿ç”¨æ··åˆç²¾åº¦
                with performance_optimizer.autocast_context():
                    y = net(x)
                    input_lengths = torch.full((batch_size,), 24, device=device, dtype=torch.long)
                    loss = ctc_loss(y, target_vector, input_lengths, target_lengths)
                
                # åå‘ä¼ æ’­ - ä½¿ç”¨ä¼˜åŒ–çš„åå‘ä¼ æ’­
                performance_optimizer.backward_step(loss, optimizer)
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)

                # æ€§èƒ½åˆ†æ
                throughput = performance_optimizer.profile_step(step_start_time, batch_size)
                
                # è®°å½•è®­ç»ƒæ­¥éª¤
                current_lr = optimizer.param_groups[0].get('lr', config.get('learning_rate', 1.0))
                training_monitor.log_step(
                    loss=loss.item(),
                    lr=current_lr,
                    batch_size=batch_size,
                    throughput=throughput
                )

                # éªŒè¯å’Œä¿å­˜
                if batch_count % config["save_per"] == 0:
                    logger.info("ğŸ” éªŒè¯æ¨¡å‹...")
                    val_accuracy = validate_model(net, validate_loader, lexicon, device)
                    logger.info(f"ğŸ“Š éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.6f} ({val_accuracy*100:.2f}%)")
                    
                    # æ€§èƒ½ç»Ÿè®¡
                    memory_info = performance_optimizer.get_memory_info()
                    if "cuda_utilization" in memory_info:
                        logger.info(f"ğŸ’¾ æ˜¾å­˜ä½¿ç”¨ç‡: {memory_info['cuda_utilization']:.1f}%")
                    
                    # æ¨¡å‹ä¿å­˜ç­–ç•¥
                    threshold = config.get("model_save_threshold", 1.0)
                    
                    if val_accuracy >= threshold:
                        # ä¿å­˜æ­£å¼æ¨¡å‹
                        model_path = f"models/model_acc{val_accuracy:.6f}_epoch{epoch}.pt"
                        torch.save(net.state_dict(), model_path)
                        logger.info(f"ğŸ‰ è¾¾åˆ°é˜ˆå€¼ {threshold:.3f}ï¼æ¨¡å‹å·²ä¿å­˜: {model_path}")
                        
                        if val_accuracy > best_accuracy:
                            best_accuracy = val_accuracy
                    else:
                        logger.info(f"ğŸ“Š æœªè¾¾åˆ°ä¿å­˜é˜ˆå€¼ {threshold:.3f}")
                        if val_accuracy > best_accuracy:
                            best_accuracy = val_accuracy
                            logger.info(f"ğŸ”„ æ›´æ–°æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.6f}")
                    
                    # ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
                    torch.save(net.state_dict(), "models/model_training.pt")
                    logger.info("ğŸ’¾ è®­ç»ƒæ£€æŸ¥ç‚¹å·²ä¿å­˜")
                    
                    # æ¸…ç†èµ„æº
                    cleanup_resources()

                batch_count += 1
                
            except Exception as e:
                logger.error(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
                cleanup_resources()
                continue
    
    # è®­ç»ƒå®Œæˆç»Ÿè®¡
    logger.info("=" * 50)
    logger.info("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    logger.info(f"ğŸ† æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.6f}")
    
    # æ€§èƒ½ç»Ÿè®¡æ‘˜è¦
    perf_summary = performance_optimizer.get_performance_summary()
    if perf_summary:
        logger.info(f"âš¡ å¹³å‡æ‰¹æ¬¡æ—¶é—´: {perf_summary.get('avg_batch_time', 0):.3f}s")
        logger.info(f"ğŸ’¾ å¹³å‡å†…å­˜ä½¿ç”¨: {perf_summary.get('avg_memory_mb', 0):.1f}MB")
    
    training_stats = training_monitor.get_training_stats()
    if training_stats:
        logger.info(f"ğŸ“Š å¹³å‡ååé‡: {training_stats.get('avg_throughput', 0):.1f} samples/s")
        logger.info(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {training_stats.get('total_time', 0):.1f}s")
    
    logger.info("=" * 50)
    
    # æœ€ç»ˆéªŒè¯
    final_accuracy = validate_model(net, validate_loader, lexicon, device)
    logger.info(f"ğŸ¯ æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_accuracy:.6f}")
    
    threshold = config.get("model_save_threshold", 1.0)
    if final_accuracy >= threshold:
        final_model_path = f"models/final_model_acc{final_accuracy:.6f}.pt"
        torch.save(net.state_dict(), final_model_path)
        logger.info(f"ğŸ‰ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    else:
        logger.info(f"ğŸ“Š æœ€ç»ˆå‡†ç¡®ç‡æœªè¾¾åˆ°ä¿å­˜é˜ˆå€¼ {threshold:.3f}")
    
    # æ˜¾ç¤ºä¼˜åŒ–å»ºè®®
    final_suggestions = performance_optimizer.suggest_optimizations()
    if final_suggestions:
        logger.info("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        for suggestion in final_suggestions:
            logger.info(f"   {suggestion}")


if __name__ == "__main__":
    train() 