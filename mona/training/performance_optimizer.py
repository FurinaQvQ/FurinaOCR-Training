"""
è®­ç»ƒæ€§èƒ½ä¼˜åŒ–å™¨
é›†æˆæ··åˆç²¾åº¦ã€æ¨¡å‹ç¼–è¯‘ã€å†…å­˜ä¼˜åŒ–ç­‰é«˜çº§æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯
"""

import torch
import torch.nn as nn
from torch.cuda.amp import GradScaler, autocast
from typing import Optional, Dict, Any
import psutil
import time
import gc
from contextlib import contextmanager

from mona.utils import logger


class PerformanceOptimizer:
    """è®­ç»ƒæ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«ç¡¬ä»¶å’Œè®­ç»ƒç›¸å…³é…ç½®
        """
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = config.get("mixed_precision", False)
        self.scaler = GradScaler() if self.use_amp else None
        
        # æ¨¡å‹ç¼–è¯‘ï¼ˆPyTorch 2.0+ï¼‰
        self.use_compile = config.get("compile_model", False)
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            "batch_times": [],
            "memory_usage": [],
            "gpu_utilization": [],
        }
        
        # CUDAä¼˜åŒ–è®¾ç½®
        if torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            if config.get("cuda_benchmark", True):
                torch.backends.cudnn.benchmark = True
                logger.info("å¯ç”¨CUDNN benchmarkæ¨¡å¼")
    
    def optimize_model(self, model: nn.Module) -> nn.Module:
        """ä¼˜åŒ–æ¨¡å‹"""
        # æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–
        if self.use_compile and hasattr(torch, 'compile'):
            try:
                model = torch.compile(model, mode='default')
                logger.info("âœ… æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å·²å¯ç”¨ (torch.compile)")
            except Exception as e:
                logger.warning(f"æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
        
        # æ··åˆç²¾åº¦ä¼˜åŒ–
        if self.use_amp:
            logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨ (AMP)")
        
        return model
    
    def optimize_dataloader(self, dataloader_kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼˜åŒ–æ•°æ®åŠ è½½å™¨è®¾ç½®"""
        if self.device.type == "cuda":
            dataloader_kwargs.update({
                "pin_memory": self.config.get("dataloader_pin_memory", True),
                "persistent_workers": self.config.get("dataloader_persistent_workers", True),
                "num_workers": min(self.config.get("dataloader_workers", 8), psutil.cpu_count()),
            })
        
        # åŠ¨æ€è°ƒæ•´batch_sizeä»¥é€‚åº”å†…å­˜
        if torch.cuda.is_available():
            available_memory = torch.cuda.get_device_properties(0).total_memory
            current_batch_size = dataloader_kwargs.get("batch_size", 128)
            
            # ç®€å•çš„å†…å­˜ä¼°ç®—å’Œæ‰¹æ¬¡å¤§å°è°ƒæ•´
            memory_per_sample = 32 * 384 * 4  # ä¼°ç®—æ¯ä¸ªæ ·æœ¬çš„å†…å­˜ä½¿ç”¨
            max_batch_size = min(available_memory // (memory_per_sample * 10), 512)  # ä¿ç•™90%å†…å­˜
            
            if current_batch_size > max_batch_size:
                dataloader_kwargs["batch_size"] = max_batch_size
                logger.warning(f"æ‰¹æ¬¡å¤§å°è°ƒæ•´ä¸º {max_batch_size} ä»¥é€‚åº”æ˜¾å­˜é™åˆ¶")
        
        return dataloader_kwargs
    
    @contextmanager
    def autocast_context(self):
        """è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if self.use_amp:
            with autocast():
                yield
        else:
            yield
    
    def backward_step(self, loss: torch.Tensor, optimizer: torch.optim.Optimizer):
        """ä¼˜åŒ–çš„åå‘ä¼ æ’­æ­¥éª¤"""
        if self.use_amp:
            self.scaler.scale(loss).backward()
            self.scaler.step(optimizer)
            self.scaler.update()
        else:
            loss.backward()
            optimizer.step()
    
    def profile_step(self, start_time: float, batch_size: int):
        """æ€§èƒ½åˆ†ææ­¥éª¤"""
        step_time = time.time() - start_time
        self.performance_stats["batch_times"].append(step_time)
        
        if torch.cuda.is_available():
            memory_mb = torch.cuda.memory_allocated() / 1024 / 1024
            self.performance_stats["memory_usage"].append(memory_mb)
        
        # è®¡ç®—ååé‡
        throughput = batch_size / step_time
        return throughput
    
    def get_memory_info(self) -> Dict[str, float]:
        """è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯"""
        info = {}
        
        # CUDAå†…å­˜
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            memory_reserved = torch.cuda.memory_reserved() / 1024**3   # GB
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            info.update({
                "cuda_allocated_gb": memory_allocated,
                "cuda_reserved_gb": memory_reserved,
                "cuda_total_gb": memory_total,
                "cuda_utilization": memory_allocated / memory_total * 100,
            })
        
        # ç³»ç»Ÿå†…å­˜
        system_memory = psutil.virtual_memory()
        info.update({
            "system_used_gb": system_memory.used / 1024**3,
            "system_total_gb": system_memory.total / 1024**3,
            "system_utilization": system_memory.percent,
        })
        
        return info
    
    def cleanup_memory(self, aggressive: bool = False):
        """å†…å­˜æ¸…ç†"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            if aggressive:
                torch.cuda.synchronize()
        
        gc.collect()
        
        if aggressive:
            # å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶
            for _ in range(3):
                gc.collect()
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.performance_stats["batch_times"]:
            return {}
        
        import numpy as np
        
        batch_times = np.array(self.performance_stats["batch_times"])
        memory_usage = np.array(self.performance_stats["memory_usage"])
        
        summary = {
            "avg_batch_time": float(np.mean(batch_times)),
            "median_batch_time": float(np.median(batch_times)),
            "max_batch_time": float(np.max(batch_times)),
            "min_batch_time": float(np.min(batch_times)),
            "std_batch_time": float(np.std(batch_times)),
        }
        
        if len(memory_usage) > 0:
            summary.update({
                "avg_memory_mb": float(np.mean(memory_usage)),
                "max_memory_mb": float(np.max(memory_usage)),
                "min_memory_mb": float(np.min(memory_usage)),
            })
        
        return summary
    
    def suggest_optimizations(self) -> list[str]:
        """åŸºäºæ€§èƒ½æ•°æ®å»ºè®®ä¼˜åŒ–æªæ–½"""
        suggestions = []
        
        memory_info = self.get_memory_info()
        perf_summary = self.get_performance_summary()
        
        # å†…å­˜ä¼˜åŒ–å»ºè®®
        if "cuda_utilization" in memory_info:
            cuda_util = memory_info["cuda_utilization"]
            if cuda_util > 90:
                suggestions.append("ğŸ”´ æ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜(>90%)ï¼Œå»ºè®®é™ä½batch_size")
            elif cuda_util < 60:
                suggestions.append("ğŸŸ¡ æ˜¾å­˜ä½¿ç”¨ç‡è¾ƒä½(<60%)ï¼Œå¯ä»¥å°è¯•å¢åŠ batch_size")
            else:
                suggestions.append("ğŸŸ¢ æ˜¾å­˜ä½¿ç”¨ç‡é€‚ä¸­")
        
        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        if not self.use_amp and torch.cuda.is_available():
            suggestions.append("ğŸ’¡ å»ºè®®å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒä»¥æå‡æ€§èƒ½")
        
        if not self.use_compile and hasattr(torch, 'compile'):
            suggestions.append("ğŸ’¡ å»ºè®®å¯ç”¨torch.compileä»¥æå‡æ¨ç†é€Ÿåº¦")
        
        # æ•°æ®åŠ è½½ä¼˜åŒ–å»ºè®®
        if perf_summary:
            avg_time = perf_summary.get("avg_batch_time", 0)
            if avg_time > 1.0:  # æ¯æ‰¹æ¬¡è¶…è¿‡1ç§’
                suggestions.append("âš¡ è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œæ£€æŸ¥æ•°æ®åŠ è½½å™¨num_workersè®¾ç½®")
        
        return suggestions


class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨"""
    
    def __init__(self, log_interval: int = 100):
        """
        åˆå§‹åŒ–è®­ç»ƒç›‘æ§å™¨
        
        Args:
            log_interval: æ—¥å¿—è®°å½•é—´éš”
        """
        self.log_interval = log_interval
        self.step_count = 0
        self.start_time = time.time()
        self.last_log_time = time.time()
        
        # ç›‘æ§æŒ‡æ ‡
        self.metrics = {
            "losses": [],
            "learning_rates": [],
            "batch_times": [],
            "throughputs": [],
        }
    
    def log_step(self, loss: float, lr: float, batch_size: int, throughput: float):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        self.step_count += 1
        current_time = time.time()
        
        # æ›´æ–°æŒ‡æ ‡
        self.metrics["losses"].append(loss)
        self.metrics["learning_rates"].append(lr)
        self.metrics["throughputs"].append(throughput)
        
        # å®šæœŸè¾“å‡ºæ—¥å¿—
        if self.step_count % self.log_interval == 0:
            elapsed_time = current_time - self.start_time
            recent_throughput = sum(self.metrics["throughputs"][-self.log_interval:]) / self.log_interval
            
            logger.info(
                f"Step {self.step_count:6d} | "
                f"Loss: {loss:.6f} | "
                f"LR: {lr:.2e} | "
                f"Throughput: {recent_throughput:.1f} samples/s | "
                f"Elapsed: {elapsed_time:.1f}s"
            )
    
    def get_training_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.metrics["losses"]:
            return {}
        
        import numpy as np
        
        return {
            "total_steps": self.step_count,
            "avg_loss": float(np.mean(self.metrics["losses"])),
            "recent_loss": float(np.mean(self.metrics["losses"][-100:])) if len(self.metrics["losses"]) >= 100 else None,
            "avg_throughput": float(np.mean(self.metrics["throughputs"])),
            "total_time": time.time() - self.start_time,
        } 