"""
ç«äº‰å¼å¹¶è¡Œæ¨¡å‹è¯„ä¼°å™¨
æ‰€æœ‰æ¨¡å‹åŒæ—¶ç«äº‰ï¼Œå‡ºé”™å³æ·˜æ±°ï¼Œæœ€ç»ˆé€‰å‡ºæœ€é²æ£’çš„æ¨¡å‹
"""

import os
import torch
import torch.nn as nn
import time
import json
import threading
import queue
from pathlib import Path
from typing import List, Dict, Any, Tuple, Set
from PIL import ImageFont
from datetime import datetime
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
import copy

import sys
sys.path.append(str(Path(__file__).parent.parent))

from mona.config import config, get_config_manager
from mona.nn.model2 import Model2
from mona.nn import predict as predict_net
from mona.text import get_lexicon
from mona.datagen.datagen import DataGen
from mona.utils import logger
import torchvision.transforms as transforms


class ModelCompetitor:
    """å•ä¸ªæ¨¡å‹çš„ç«äº‰è€…"""
    
    def __init__(self, model_path: Path, model_id: int, device: str):
        self.model_path = model_path
        self.model_name = model_path.stem
        self.model_id = model_id
        self.device = device
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.correct_count = 0
        self.total_count = 0
        self.error_count = 0
        self.inference_times = []
        self.is_active = True
        self.elimination_reason = None
        self.elimination_time = None
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()
        
    def _load_model(self) -> nn.Module:
        """åŠ è½½æ¨¡å‹"""
        try:
            lexicon = get_lexicon()
            net = Model2(lexicon.lexicon_size(), 1).to(self.device)
            state_dict = torch.load(self.model_path, map_location=self.device, weights_only=True)
            net.load_state_dict(state_dict)
            net.eval()
            return net
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥ {self.model_name}: {e}")
            self.is_active = False
            self.elimination_reason = f"åŠ è½½å¤±è´¥: {e}"
            return None
    
    def predict(self, tensor: torch.Tensor, lexicon) -> Tuple[str, float]:
        """æ¨¡å‹é¢„æµ‹"""
        if not self.is_active or self.model is None:
            return "", 0.0
        
        try:
            start_time = time.time()
            with torch.no_grad():
                predicted = predict_net(self.model, tensor, lexicon)
            inference_time = time.time() - start_time
            
            self.inference_times.append(inference_time)
            pred_text = predicted[0] if predicted else ""
            return pred_text, inference_time
            
        except Exception as e:
            self.eliminate(f"é¢„æµ‹é”™è¯¯: {e}")
            return "", 0.0
    
    def update_stats(self, is_correct: bool):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_active:
            return
            
        self.total_count += 1
        if is_correct:
            self.correct_count += 1
        else:
            self.error_count += 1
    
    def eliminate(self, reason: str):
        """æ·˜æ±°æ¨¡å‹"""
        self.is_active = False
        self.elimination_reason = reason
        self.elimination_time = datetime.now()
        
        # é‡Šæ”¾GPUå†…å­˜
        if self.model is not None:
            del self.model
            torch.cuda.empty_cache()
    
    @property
    def accuracy(self) -> float:
        """å½“å‰å‡†ç¡®ç‡"""
        return self.correct_count / self.total_count if self.total_count > 0 else 0.0
    
    @property
    def avg_inference_time(self) -> float:
        """å¹³å‡æ¨ç†æ—¶é—´"""
        return np.mean(self.inference_times) if self.inference_times else 0.0
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "model_id": self.model_id,
            "is_active": self.is_active,
            "correct_count": self.correct_count,
            "total_count": self.total_count,
            "error_count": self.error_count,
            "accuracy": self.accuracy,
            "avg_inference_time_ms": self.avg_inference_time * 1000,
            "elimination_reason": self.elimination_reason,
            "elimination_time": self.elimination_time.isoformat() if self.elimination_time else None
        }


class CompetitiveEvaluator:
    """ç«äº‰å¼å¹¶è¡Œæ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self, models_dir: str = "models", max_workers: int = None):
        self.models_dir = Path(models_dir)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.lexicon = get_lexicon()
        self.config_manager = get_config_manager()
        
        # å¹¶è¡Œé…ç½®
        self.max_workers = max_workers or min(torch.cuda.device_count() if torch.cuda.is_available() else 1, 4)
        
        # åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨
        fonts = [ImageFont.truetype("./assets/genshin.ttf", i) for i in range(15, 90)]
        self.datagen = DataGen(config, fonts, self.lexicon)
        
        # ç«äº‰è€…åˆ—è¡¨
        self.competitors: List[ModelCompetitor] = []
        self.data_queue = queue.Queue(maxsize=1000)  # æ•°æ®é˜Ÿåˆ—
        self.stop_generation = threading.Event()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.start_time = None
        self.total_samples_generated = 0
        self.evaluation_log = []
        
        logger.info(f"ğŸ ç«äº‰å¼è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ® è®¾å¤‡: {self.device}")
        logger.info(f"ğŸ”¥ æœ€å¤§å¹¶è¡Œæ•°: {self.max_workers}")
    
    def discover_and_load_models(self) -> List[ModelCompetitor]:
        """å‘ç°å¹¶åŠ è½½æ‰€æœ‰æ¨¡å‹"""
        if not self.models_dir.exists():
            logger.error(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.models_dir}")
            return []
        
        model_files = list(self.models_dir.glob("*.pt"))
        
        # è¿‡æ»¤æ‰ä¸´æ—¶æ–‡ä»¶
        filtered_models = []
        for model_file in model_files:
            if not any(skip in model_file.name.lower() for skip in 
                      ["training", "checkpoint", "temp", "tmp"]):
                filtered_models.append(model_file)
        
        logger.info(f"ğŸ“‚ å‘ç°æ¨¡å‹æ–‡ä»¶: {len(filtered_models)} ä¸ª")
        
        # åˆ›å»ºç«äº‰è€…
        competitors = []
        for i, model_path in enumerate(filtered_models):
            competitor = ModelCompetitor(model_path, i, self.device)
            if competitor.is_active:
                competitors.append(competitor)
                size_mb = model_path.stat().st_size / 1024 / 1024
                logger.info(f"   ğŸ¯ åŠ è½½æˆåŠŸ: {competitor.model_name} ({size_mb:.1f}MB)")
            else:
                logger.error(f"   âŒ åŠ è½½å¤±è´¥: {model_path.name}")
        
        logger.info(f"ğŸ å‚èµ›æ¨¡å‹æ•°: {len(competitors)}")
        return competitors
    
    def data_generator_worker(self):
        """æ•°æ®ç”Ÿæˆå™¨å·¥ä½œçº¿ç¨‹"""
        batch_size = 50  # æ¯æ‰¹ç”Ÿæˆ50ä¸ªæ ·æœ¬
        
        while not self.stop_generation.is_set():
            try:
                # ç”Ÿæˆä¸€æ‰¹æ•°æ®
                batch_data = []
                for _ in range(batch_size):
                    if self.stop_generation.is_set():
                        break
                    
                    im, text = self.datagen.generate_image()
                    tensor = transforms.ToTensor()(im).unsqueeze(0).to(self.device)
                    batch_data.append((tensor, text))
                    self.total_samples_generated += 1
                
                # å°†æ•°æ®æ”¾å…¥é˜Ÿåˆ—
                if batch_data:
                    self.data_queue.put(batch_data, timeout=1)
                    
            except queue.Full:
                continue
            except Exception as e:
                logger.error(f"âŒ æ•°æ®ç”Ÿæˆé”™è¯¯: {e}")
                time.sleep(0.1)
    
    def evaluate_competitor_batch(self, competitor: ModelCompetitor, data_batch: List[Tuple[torch.Tensor, str]]) -> bool:
        """è¯„ä¼°å•ä¸ªç«äº‰è€…çš„ä¸€æ‰¹æ•°æ®"""
        if not competitor.is_active:
            return False
        
        for tensor, true_text in data_batch:
            if not competitor.is_active:
                break
            
            try:
                # é¢„æµ‹
                pred_text, inference_time = competitor.predict(tensor, self.lexicon)
                
                # æ£€æŸ¥ç»“æœ
                is_correct = (pred_text == true_text)
                competitor.update_stats(is_correct)
                
                # å¦‚æœé¢„æµ‹é”™è¯¯ï¼Œæ·˜æ±°è¯¥æ¨¡å‹
                if not is_correct:
                    competitor.eliminate(f"é¢„æµ‹é”™è¯¯: çœŸå®='{true_text}', é¢„æµ‹='{pred_text}'")
                    return False
                    
            except Exception as e:
                competitor.eliminate(f"è¯„ä¼°å¼‚å¸¸: {e}")
                return False
        
        return True
    
    def run_competitive_evaluation(self, min_survival_time: int = 60, 
                                 max_evaluation_time: int = 1800) -> Dict[str, Any]:
        """è¿è¡Œç«äº‰å¼è¯„ä¼°"""
        logger.info("ğŸš€ å¼€å§‹ç«äº‰å¼æ¨¡å‹è¯„ä¼°...")
        logger.info("ğŸ è§„åˆ™: é¢„æµ‹é”™è¯¯å³æ·˜æ±°ï¼Œæœ€åå­˜æ´»çš„æ¨¡å‹è·èƒœ")
        logger.info("=" * 60)
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹
        self.competitors = self.discover_and_load_models()
        if not self.competitors:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„ç«äº‰æ¨¡å‹")
            return {}
        
        self.start_time = time.time()
        
        # å¯åŠ¨æ•°æ®ç”Ÿæˆçº¿ç¨‹
        logger.info("ğŸŒŠ å¯åŠ¨åœ¨çº¿æ•°æ®ç”Ÿæˆ...")
        data_thread = threading.Thread(target=self.data_generator_worker, daemon=True)
        data_thread.start()
        
        # æ˜¾ç¤ºåˆå§‹çŠ¶æ€
        logger.info(f"ğŸ® å‚èµ›é€‰æ‰‹:")
        for competitor in self.competitors:
            logger.info(f"   ğŸ¯ {competitor.model_name}")
        
        logger.info(f"\nâ±ï¸ å¼€å§‹æ— é™è¯„ä¼°...")
        
        try:
            # ä¸»è¯„ä¼°å¾ªç¯
            while True:
                current_time = time.time()
                elapsed_time = current_time - self.start_time
                
                # æ£€æŸ¥æ—¶é—´é™åˆ¶
                if elapsed_time > max_evaluation_time:
                    logger.info(f"â° è¾¾åˆ°æœ€å¤§è¯„ä¼°æ—¶é—´ {max_evaluation_time}sï¼Œåœæ­¢è¯„ä¼°")
                    break
                
                # è·å–æ´»è·ƒçš„ç«äº‰è€…
                active_competitors = [c for c in self.competitors if c.is_active]
                
                if len(active_competitors) == 0:
                    logger.info("ğŸ’¥ æ‰€æœ‰æ¨¡å‹éƒ½è¢«æ·˜æ±°äº†ï¼")
                    break
                elif len(active_competitors) == 1 and elapsed_time >= min_survival_time:
                    logger.info(f"ğŸ† æ‰¾åˆ°æœ€ç»ˆè·èƒœè€…ï¼")
                    break
                
                # å°è¯•è·å–æ•°æ®æ‰¹æ¬¡
                try:
                    data_batch = self.data_queue.get(timeout=1)
                except queue.Empty:
                    continue
                
                # å¹¶è¡Œè¯„ä¼°æ´»è·ƒçš„ç«äº‰è€…
                with ThreadPoolExecutor(max_workers=min(len(active_competitors), self.max_workers)) as executor:
                    future_to_competitor = {
                        executor.submit(self.evaluate_competitor_batch, competitor, data_batch): competitor
                        for competitor in active_competitors
                    }
                    
                    # å¤„ç†ç»“æœ
                    eliminated_this_round = []
                    for future in as_completed(future_to_competitor):
                        competitor = future_to_competitor[future]
                        try:
                            success = future.result()
                            if not success and competitor.is_active:
                                eliminated_this_round.append(competitor)
                        except Exception as e:
                            competitor.eliminate(f"æ‰§è¡Œå¼‚å¸¸: {e}")
                            eliminated_this_round.append(competitor)
                
                # æŠ¥å‘Šæ·˜æ±°æƒ…å†µ
                if eliminated_this_round:
                    for competitor in eliminated_this_round:
                        logger.info(f"ğŸ’¥ æ·˜æ±°: {competitor.model_name} - {competitor.elimination_reason}")
                        logger.info(f"     å­˜æ´»æ ·æœ¬: {competitor.correct_count:,}, å‡†ç¡®ç‡: {competitor.accuracy:.6f}")
                
                # å®šæœŸçŠ¶æ€æŠ¥å‘Š
                if int(elapsed_time) % 10 == 0 and len(active_competitors) > 1:
                    self._print_status_report(elapsed_time, active_competitors)
        
        except KeyboardInterrupt:
            logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
        finally:
            # åœæ­¢æ•°æ®ç”Ÿæˆ
            self.stop_generation.set()
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        return self._generate_final_report()
    
    def _print_status_report(self, elapsed_time: float, active_competitors: List[ModelCompetitor]):
        """æ‰“å°çŠ¶æ€æŠ¥å‘Š"""
        logger.info(f"\nğŸ“Š ç¬¬ {int(elapsed_time)}s çŠ¶æ€æŠ¥å‘Š:")
        logger.info(f"ğŸ å‰©ä½™é€‰æ‰‹: {len(active_competitors)}")
        
        # æŒ‰æ­£ç¡®æ•°é‡æ’åº
        sorted_competitors = sorted(active_competitors, key=lambda x: x.correct_count, reverse=True)
        
        for i, competitor in enumerate(sorted_competitors[:5], 1):  # åªæ˜¾ç¤ºå‰5å
            logger.info(f"   {i}. {competitor.model_name}: "
                       f"{competitor.correct_count:,}æ ·æœ¬, "
                       f"å‡†ç¡®ç‡{competitor.accuracy:.6f}, "
                       f"é€Ÿåº¦{competitor.avg_inference_time*1000:.1f}ms")
        
        logger.info(f"ğŸ“ˆ æ€»æ ·æœ¬å·²ç”Ÿæˆ: {self.total_samples_generated:,}")
    
    def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æŠ¥å‘Š"""
        end_time = time.time()
        total_time = end_time - self.start_time if self.start_time else 0
        
        # è·å–æ‰€æœ‰ç«äº‰è€…çš„ç»Ÿè®¡ä¿¡æ¯
        all_stats = [competitor.get_stats() for competitor in self.competitors]
        
        # æ‰¾åˆ°è·èƒœè€…ï¼ˆæ´»è·ƒä¸”æ­£ç¡®æ•°é‡æœ€å¤šçš„ï¼‰
        active_competitors = [c for c in self.competitors if c.is_active]
        winner = None
        
        if active_competitors:
            winner = max(active_competitors, key=lambda x: x.correct_count)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "evaluation_summary": {
                "total_competitors": len(self.competitors),
                "final_survivors": len(active_competitors),
                "winner": winner.model_name if winner else None,
                "total_evaluation_time": total_time,
                "total_samples_generated": self.total_samples_generated,
                "evaluation_mode": "competitive_elimination",
                "evaluation_time": datetime.now().isoformat()
            },
            "winner_details": winner.get_stats() if winner else None,
            "all_competitors": all_stats,
            "elimination_timeline": [
                {
                    "model_name": c.model_name,
                    "elimination_time": c.elimination_time.isoformat() if c.elimination_time else None,
                    "elimination_reason": c.elimination_reason,
                    "survival_samples": c.correct_count,
                    "accuracy": c.accuracy
                }
                for c in self.competitors if not c.is_active
            ]
        }
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        logger.info("\n" + "ğŸ†" * 30)
        logger.info("ğŸ‰ ç«äº‰å¼è¯„ä¼°å®Œæˆ!")
        logger.info("ğŸ†" * 30)
        
        if winner:
            logger.info(f"\nğŸ¥‡ æœ€ç»ˆè·èƒœè€…: {winner.model_name}")
            logger.info(f"   ğŸ¯ å­˜æ´»æ ·æœ¬æ•°: {winner.correct_count:,}")
            logger.info(f"   ğŸ“Š å‡†ç¡®ç‡: {winner.accuracy:.6f}")
            logger.info(f"   âš¡ å¹³å‡é€Ÿåº¦: {winner.avg_inference_time*1000:.2f}ms")
            logger.info(f"   â±ï¸ æ€»è¯„ä¼°æ—¶é—´: {total_time:.1f}s")
        else:
            logger.info("ğŸ’¥ æ²¡æœ‰æ¨¡å‹å­˜æ´»åˆ°æœ€å!")
        
        logger.info(f"\nğŸ“ˆ è¯„ä¼°ç»Ÿè®¡:")
        logger.info(f"   ğŸ® å‚èµ›æ¨¡å‹: {len(self.competitors)}")
        logger.info(f"   ğŸ æœ€ç»ˆå­˜æ´»: {len(active_competitors)}")
        logger.info(f"   ğŸ“Š æ€»æ ·æœ¬æ•°: {self.total_samples_generated:,}")
        logger.info(f"   â±ï¸ æ€»ç”¨æ—¶: {total_time:.1f}s")
        
        # æ˜¾ç¤ºæ·˜æ±°é¡ºåº
        eliminated = [c for c in self.competitors if not c.is_active]
        if eliminated:
            logger.info(f"\nğŸ’¥ æ·˜æ±°é¡ºåº (ä»æ—©åˆ°æ™š):")
            eliminated_sorted = sorted(eliminated, 
                                     key=lambda x: x.elimination_time if x.elimination_time else datetime.min)
            
            for i, competitor in enumerate(eliminated_sorted, 1):
                elapsed = (competitor.elimination_time - datetime.fromtimestamp(self.start_time)).total_seconds() if competitor.elimination_time else 0
                logger.info(f"   {i}. {competitor.model_name} "
                           f"(ç¬¬{elapsed:.1f}sæ·˜æ±°, å­˜æ´»{competitor.correct_count:,}æ ·æœ¬)")
        
        return report
    
    def save_report(self, report: Dict[str, Any], output_dir: str = "logs") -> str:
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = output_dir / f"competitive_evaluation_{timestamp}.json"
        
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return str(report_file)
    
    def copy_winner_model(self, report: Dict[str, Any]) -> str:
        """å¤åˆ¶è·èƒœæ¨¡å‹åˆ°models/modelsæ–‡ä»¶å¤¹ï¼Œä¿æŒåŸæ–‡ä»¶åï¼Œå¹¶è‡ªåŠ¨å¯¼å‡ºONNXå’Œindex_2_word.json"""
        winner_name = report["evaluation_summary"].get("winner")
        if not winner_name:
            logger.error("âŒ æ²¡æœ‰è·èƒœè€…ï¼Œæ— æ³•å¤åˆ¶æ¨¡å‹")
            return None
        
        # æ‰¾åˆ°è·èƒœè€…çš„æ¨¡å‹æ–‡ä»¶
        winner_competitor = None
        for competitor in self.competitors:
            if competitor.model_name == winner_name:
                winner_competitor = competitor
                break
        
        if not winner_competitor:
            logger.error(f"âŒ æ‰¾ä¸åˆ°è·èƒœè€…æ¨¡å‹: {winner_name}")
            return None
        
        # åˆ›å»ºç›®æ ‡ç›®å½• models/models
        import shutil
        from pathlib import Path
        target_dir = Path("models") / "models"
        target_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿æŒåŸå§‹æ–‡ä»¶å
        source_path = winner_competitor.model_path
        target_path = target_dir / source_path.name
        
        # å¤åˆ¶æ¨¡å‹
        shutil.copy2(source_path, target_path)
        logger.info(f"ğŸ† å† å†›æ¨¡å‹å·²å¤åˆ¶: {target_path}")
        logger.info(f"ğŸ“‚ æºæ–‡ä»¶: {source_path}")
        logger.info(f"ğŸ“Š æ¨¡å‹æ€§èƒ½: å­˜æ´»{winner_competitor.correct_count:,}æ ·æœ¬, "
                    f"å‡†ç¡®ç‡{winner_competitor.accuracy:.6f}")
        
        # ========== è‡ªåŠ¨å¯¼å‡ºONNX ==========
        try:
            from mona.nn.model2 import Model2
            from mona.text import get_lexicon
            import torch
            lexicon = get_lexicon()
            device = "cuda" if torch.cuda.is_available() else "cpu"
            net = Model2(lexicon.lexicon_size(), 1).to(device)
            net.load_state_dict(torch.load(target_path, map_location=device, weights_only=True))
            net.eval()
            dummy_input = torch.randn(1, 1, 32, 384).to(device)
            onnx_path = target_dir / "model_training.onnx"
            torch.onnx.export(
                net,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
            )
            logger.info(f"âœ… å·²å¯¼å‡ºONNXæ¨¡å‹: {onnx_path}")
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºONNXå¤±è´¥: {e}")
        
        # ========== å¯¼å‡ºindex_2_word.json ==========
        try:
            index2word = get_lexicon().index_to_word
            # è½¬ä¸ºstr-keyçš„dict
            index2word_str = {str(k): v for k, v in index2word.items()}
            json_path = target_dir / "index_2_word.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(index2word_str, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… å·²å¯¼å‡ºindex_2_word.json: {json_path}")
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºindex_2_word.jsonå¤±è´¥: {e}")
        
        return str(target_path)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç«äº‰å¼å¹¶è¡Œæ¨¡å‹è¯„ä¼°å™¨")
    parser.add_argument("--models-dir", default="models", help="æ¨¡å‹æ–‡ä»¶ç›®å½•")
    parser.add_argument("--min-survival-time", type=int, default=60, help="æœ€å°å­˜æ´»æ—¶é—´(ç§’)")
    parser.add_argument("--max-evaluation-time", type=int, default=1800, help="æœ€å¤§è¯„ä¼°æ—¶é—´(ç§’)")
    parser.add_argument("--max-workers", type=int, default=None, help="æœ€å¤§å¹¶è¡Œå·¥ä½œæ•°")
    parser.add_argument("--copy-winner", action="store_true", help="å¤åˆ¶è·èƒœæ¨¡å‹åˆ°models/modelsæ–‡ä»¶å¤¹")
    parser.add_argument("--output-dir", default="logs", help="æŠ¥å‘Šè¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = CompetitiveEvaluator(args.models_dir, args.max_workers)
    
    # è¿è¡Œç«äº‰å¼è¯„ä¼°
    report = evaluator.run_competitive_evaluation(
        min_survival_time=args.min_survival_time,
        max_evaluation_time=args.max_evaluation_time
    )
    
    if report:
        # ä¿å­˜æŠ¥å‘Š
        evaluator.save_report(report, args.output_dir)
        
        # å¤åˆ¶è·èƒœæ¨¡å‹
        if args.copy_winner:
            evaluator.copy_winner_model(report)
    
    return 0


if __name__ == "__main__":
    exit(main()) 