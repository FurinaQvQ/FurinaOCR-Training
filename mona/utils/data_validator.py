"""
æ•°æ®è´¨é‡éªŒè¯å™¨
éªŒè¯è®­ç»ƒæ•°æ®çš„å®Œæ•´æ€§ã€æ ¼å¼æ­£ç¡®æ€§å’Œè´¨é‡æŒ‡æ ‡
"""

import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import json
from collections import Counter
import cv2

from mona.utils import logger


class DataQualityValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨"""
    
    def __init__(self, output_dir: str = "validation_reports"):
        """
        åˆå§‹åŒ–æ•°æ®è´¨é‡éªŒè¯å™¨
        
        Args:
            output_dir: éªŒè¯æŠ¥å‘Šè¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.validation_results = {
            'image_stats': {},
            'text_stats': {},
            'quality_issues': [],
            'recommendations': []
        }
    
    def validate_image_batch(self, images: torch.Tensor) -> Dict[str, Any]:
        """
        éªŒè¯å›¾åƒæ‰¹æ¬¡çš„è´¨é‡
        
        Args:
            images: å›¾åƒå¼ é‡ (B, C, H, W)
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        batch_size, channels, height, width = images.shape
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œåˆ†æ
        images_np = images.detach().cpu().numpy()
        
        # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'batch_size': batch_size,
            'shape': (channels, height, width),
            'dtype': str(images.dtype),
            'device': str(images.device),
            'mean': float(np.mean(images_np)),
            'std': float(np.std(images_np)),
            'min': float(np.min(images_np)),
            'max': float(np.max(images_np)),
        }
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        issues = []
        
        # æ£€æŸ¥å€¼åŸŸ
        if stats['min'] < -1.0 or stats['max'] > 1.0:
            issues.append("å›¾åƒåƒç´ å€¼è¶…å‡ºé¢„æœŸèŒƒå›´ [-1, 1]")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if np.isnan(images_np).any():
            issues.append("æ£€æµ‹åˆ°NaNå€¼")
        
        if np.isinf(images_np).any():
            issues.append("æ£€æµ‹åˆ°Infå€¼")
        
        # æ£€æŸ¥å¯¹æ¯”åº¦
        contrast_scores = []
        for i in range(batch_size):
            img = images_np[i, 0]  # å‡è®¾æ˜¯ç°åº¦å›¾
            contrast = np.std(img)
            contrast_scores.append(contrast)
        
        avg_contrast = np.mean(contrast_scores)
        if avg_contrast < 0.1:
            issues.append(f"å¹³å‡å¯¹æ¯”åº¦è¿‡ä½: {avg_contrast:.3f}")
        
        # æ£€æŸ¥äº®åº¦åˆ†å¸ƒ
        brightness_scores = [np.mean(images_np[i, 0]) for i in range(batch_size)]
        brightness_std = np.std(brightness_scores)
        
        if brightness_std > 0.5:
            issues.append(f"äº®åº¦åˆ†å¸ƒä¸å‡åŒ€: std={brightness_std:.3f}")
        
        stats['contrast_scores'] = contrast_scores
        stats['brightness_scores'] = brightness_scores
        stats['quality_issues'] = issues
        
        return stats
    
    def validate_text_batch(self, texts: List[str]) -> Dict[str, Any]:
        """
        éªŒè¯æ–‡æœ¬æ‰¹æ¬¡çš„è´¨é‡
        
        Args:
            texts: æ–‡æœ¬å­—ç¬¦ä¸²åˆ—è¡¨
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        if not texts:
            return {'error': 'æ–‡æœ¬åˆ—è¡¨ä¸ºç©º'}
        
        # åŸºæœ¬ç»Ÿè®¡
        text_lengths = [len(text) for text in texts]
        
        stats = {
            'batch_size': len(texts),
            'total_chars': sum(text_lengths),
            'avg_length': np.mean(text_lengths),
            'min_length': min(text_lengths),
            'max_length': max(text_lengths),
            'length_std': np.std(text_lengths),
        }
        
        # å­—ç¬¦é¢‘ç‡ç»Ÿè®¡
        all_chars = ''.join(texts)
        char_counter = Counter(all_chars)
        
        stats['unique_chars'] = len(char_counter)
        stats['char_frequency'] = dict(char_counter.most_common(20))
        
        # æ£€æŸ¥å¼‚å¸¸
        issues = []
        
        # æ£€æŸ¥ç©ºå­—ç¬¦ä¸²
        empty_count = sum(1 for text in texts if len(text) == 0)
        if empty_count > 0:
            issues.append(f"å‘ç° {empty_count} ä¸ªç©ºå­—ç¬¦ä¸²")
        
        # æ£€æŸ¥é•¿åº¦å¼‚å¸¸
        if stats['length_std'] > stats['avg_length'] * 0.5:
            issues.append("æ–‡æœ¬é•¿åº¦å·®å¼‚è¿‡å¤§")
        
        # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦
        unusual_chars = set()
        for char in char_counter:
            if ord(char) < 32 or ord(char) > 126:  # éå¯æ‰“å°ASCIIå­—ç¬¦
                if char not in ['\n', '\t', ' ']:  # æ’é™¤å¸¸è§ç©ºç™½å­—ç¬¦
                    unusual_chars.add(char)
        
        if unusual_chars:
            issues.append(f"å‘ç°å¼‚å¸¸å­—ç¬¦: {list(unusual_chars)[:10]}")  # åªæ˜¾ç¤ºå‰10ä¸ª
        
        stats['quality_issues'] = issues
        
        return stats
    
    def validate_image_text_pairs(self, 
                                 images: torch.Tensor, 
                                 texts: List[str]) -> Dict[str, Any]:
        """
        éªŒè¯å›¾åƒ-æ–‡æœ¬å¯¹çš„ä¸€è‡´æ€§
        
        Args:
            images: å›¾åƒå¼ é‡
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        if len(images) != len(texts):
            return {'error': f'å›¾åƒæ•°é‡({len(images)})ä¸æ–‡æœ¬æ•°é‡({len(texts)})ä¸åŒ¹é…'}
        
        batch_size = len(images)
        
        # éªŒè¯å›¾åƒå’Œæ–‡æœ¬çš„å¤æ‚åº¦ç›¸å…³æ€§
        image_complexities = []
        text_complexities = []
        
        images_np = images.detach().cpu().numpy()
        
        for i in range(batch_size):
            # å›¾åƒå¤æ‚åº¦ï¼ˆä½¿ç”¨è¾¹ç¼˜æ£€æµ‹ï¼‰
            img = images_np[i, 0]  # ç°åº¦å›¾
            img_uint8 = ((img + 1) * 127.5).astype(np.uint8)  # è½¬æ¢åˆ°0-255
            edges = cv2.Canny(img_uint8, 50, 150)
            img_complexity = np.sum(edges > 0) / (img.shape[0] * img.shape[1])
            image_complexities.append(img_complexity)
            
            # æ–‡æœ¬å¤æ‚åº¦ï¼ˆå­—ç¬¦ç§ç±»æ•°ï¼‰
            text_complexity = len(set(texts[i])) / max(len(texts[i]), 1)
            text_complexities.append(text_complexity)
        
        # è®¡ç®—ç›¸å…³æ€§
        if len(image_complexities) > 1:
            correlation = np.corrcoef(image_complexities, text_complexities)[0, 1]
        else:
            correlation = 0.0
        
        stats = {
            'batch_size': batch_size,
            'image_complexities': image_complexities,
            'text_complexities': text_complexities,
            'complexity_correlation': float(correlation) if not np.isnan(correlation) else 0.0,
            'avg_image_complexity': np.mean(image_complexities),
            'avg_text_complexity': np.mean(text_complexities),
        }
        
        # æ£€æŸ¥å¼‚å¸¸
        issues = []
        
        if abs(correlation) < 0.1:
            issues.append("å›¾åƒå¤æ‚åº¦ä¸æ–‡æœ¬å¤æ‚åº¦ç›¸å…³æ€§è¾ƒä½")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒè¿‡äºç®€å•æˆ–å¤æ‚
        simple_images = sum(1 for x in image_complexities if x < 0.01)
        complex_images = sum(1 for x in image_complexities if x > 0.5)
        
        if simple_images > batch_size * 0.2:
            issues.append(f"è¿‡å¤šç®€å•å›¾åƒ: {simple_images}/{batch_size}")
        
        if complex_images > batch_size * 0.1:
            issues.append(f"è¿‡å¤šå¤æ‚å›¾åƒ: {complex_images}/{batch_size}")
        
        stats['quality_issues'] = issues
        
        return stats
    
    def generate_quality_report(self, 
                              validation_data: List[Tuple[torch.Tensor, List[str]]], 
                              sample_count: int = 100) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
        
        Args:
            validation_data: éªŒè¯æ•°æ®åˆ—è¡¨ [(images, texts), ...]
            sample_count: é‡‡æ ·æ•°é‡
            
        Returns:
            å®Œæ•´çš„è´¨é‡æŠ¥å‘Š
        """
        logger.info(f"å¼€å§‹ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Šï¼Œé‡‡æ ·æ•°é‡: {sample_count}")
        
        all_image_stats = []
        all_text_stats = []
        all_pair_stats = []
        
        sample_indices = np.random.choice(len(validation_data), 
                                        min(sample_count, len(validation_data)), 
                                        replace=False)
        
        for idx in sample_indices:
            images, texts = validation_data[idx]
            
            # éªŒè¯å›¾åƒ
            img_stats = self.validate_image_batch(images)
            all_image_stats.append(img_stats)
            
            # éªŒè¯æ–‡æœ¬
            text_stats = self.validate_text_batch(texts)
            all_text_stats.append(text_stats)
            
            # éªŒè¯å›¾åƒ-æ–‡æœ¬å¯¹
            pair_stats = self.validate_image_text_pairs(images, texts)
            all_pair_stats.append(pair_stats)
        
        # æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        report = self._aggregate_validation_results(all_image_stats, all_text_stats, all_pair_stats)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"quality_report_{len(sample_indices)}_samples.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report
    
    def _aggregate_validation_results(self, 
                                    image_stats: List[Dict], 
                                    text_stats: List[Dict], 
                                    pair_stats: List[Dict]) -> Dict[str, Any]:
        """æ±‡æ€»éªŒè¯ç»“æœ"""
        
        # æ±‡æ€»å›¾åƒç»Ÿè®¡
        image_summary = {
            'total_batches': len(image_stats),
            'avg_mean': np.mean([s['mean'] for s in image_stats]),
            'avg_std': np.mean([s['std'] for s in image_stats]),
            'avg_contrast': np.mean([np.mean(s['contrast_scores']) for s in image_stats]),
            'total_issues': sum(len(s['quality_issues']) for s in image_stats),
        }
        
        # æ±‡æ€»æ–‡æœ¬ç»Ÿè®¡
        text_summary = {
            'total_batches': len(text_stats),
            'avg_length': np.mean([s['avg_length'] for s in text_stats if 'avg_length' in s]),
            'total_chars': sum(s['total_chars'] for s in text_stats if 'total_chars' in s),
            'total_issues': sum(len(s['quality_issues']) for s in text_stats if 'quality_issues' in s),
        }
        
        # æ±‡æ€»é…å¯¹ç»Ÿè®¡
        pair_summary = {
            'total_pairs': len(pair_stats),
            'avg_correlation': np.mean([s['complexity_correlation'] for s in pair_stats]),
            'total_issues': sum(len(s['quality_issues']) for s in pair_stats if 'quality_issues' in s),
        }
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(image_summary, text_summary, pair_summary)
        
        return {
            'timestamp': np.datetime64('now').astype(str),
            'image_summary': image_summary,
            'text_summary': text_summary,
            'pair_summary': pair_summary,
            'recommendations': recommendations,
            'overall_score': self._calculate_overall_score(image_summary, text_summary, pair_summary)
        }
    
    def _generate_recommendations(self, 
                                image_summary: Dict, 
                                text_summary: Dict, 
                                pair_summary: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # å›¾åƒç›¸å…³å»ºè®®
        if image_summary['avg_contrast'] < 0.15:
            recommendations.append("ğŸ”§ å»ºè®®å¢å¼ºå›¾åƒå¯¹æ¯”åº¦ä»¥æé«˜OCRè¯†åˆ«ç‡")
        
        if image_summary['total_issues'] > image_summary['total_batches'] * 0.1:
            recommendations.append("âš ï¸ å›¾åƒè´¨é‡é—®é¢˜è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥æ•°æ®ç”Ÿæˆæµç¨‹")
        
        # æ–‡æœ¬ç›¸å…³å»ºè®®
        if text_summary['avg_length'] < 3:
            recommendations.append("ğŸ“ æ–‡æœ¬é•¿åº¦åçŸ­ï¼Œå¯èƒ½å½±å“æ¨¡å‹å­¦ä¹ ")
        
        if text_summary['avg_length'] > 50:
            recommendations.append("ğŸ“ æ–‡æœ¬é•¿åº¦åé•¿ï¼Œè€ƒè™‘æ˜¯å¦éœ€è¦æˆªæ–­")
        
        # é…å¯¹ç›¸å…³å»ºè®®
        if pair_summary['avg_correlation'] < 0.1:
            recommendations.append("ğŸ”— å›¾åƒ-æ–‡æœ¬å¤æ‚åº¦ç›¸å…³æ€§ä½ï¼Œæ£€æŸ¥æ•°æ®ç”Ÿæˆé€»è¾‘")
        
        if len(recommendations) == 0:
            recommendations.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
        
        return recommendations
    
    def _calculate_overall_score(self, 
                               image_summary: Dict, 
                               text_summary: Dict, 
                               pair_summary: Dict) -> float:
        """è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•° (0-100)"""
        score = 100.0
        
        # å›¾åƒè´¨é‡æ‰£åˆ†
        if image_summary['avg_contrast'] < 0.1:
            score -= 20
        elif image_summary['avg_contrast'] < 0.15:
            score -= 10
        
        # æ–‡æœ¬è´¨é‡æ‰£åˆ†
        if text_summary['avg_length'] < 2:
            score -= 15
        elif text_summary['avg_length'] > 100:
            score -= 10
        
        # é…å¯¹è´¨é‡æ‰£åˆ†
        if pair_summary['avg_correlation'] < 0:
            score -= 15
        elif pair_summary['avg_correlation'] < 0.1:
            score -= 10
        
        # é—®é¢˜æ•°é‡æ‰£åˆ†
        total_batches = max(image_summary['total_batches'], 1)
        issue_rate = (image_summary['total_issues'] + text_summary['total_issues'] + 
                     pair_summary['total_issues']) / (total_batches * 3)
        
        score -= min(issue_rate * 50, 30)  # æœ€å¤šæ‰£30åˆ†
        
        return max(score, 0.0)
    
    def visualize_quality_metrics(self, report: Dict[str, Any]) -> str:
        """
        å¯è§†åŒ–è´¨é‡æŒ‡æ ‡
        
        Args:
            report: è´¨é‡æŠ¥å‘Š
            
        Returns:
            å¯è§†åŒ–æ–‡ä»¶è·¯å¾„
        """
        try:
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            fig.suptitle('æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š', fontsize=16)
            
            # æ•´ä½“åˆ†æ•°
            ax1 = axes[0, 0]
            score = report['overall_score']
            colors = ['red' if score < 60 else 'yellow' if score < 80 else 'green']
            ax1.bar(['æ•´ä½“è´¨é‡åˆ†æ•°'], [score], color=colors)
            ax1.set_ylim(0, 100)
            ax1.set_ylabel('åˆ†æ•°')
            ax1.set_title(f'æ•´ä½“è´¨é‡åˆ†æ•°: {score:.1f}')
            
            # é—®é¢˜ç»Ÿè®¡
            ax2 = axes[0, 1]
            issue_counts = [
                report['image_summary']['total_issues'],
                report['text_summary']['total_issues'],
                report['pair_summary']['total_issues']
            ]
            ax2.bar(['å›¾åƒ', 'æ–‡æœ¬', 'é…å¯¹'], issue_counts)
            ax2.set_ylabel('é—®é¢˜æ•°é‡')
            ax2.set_title('è´¨é‡é—®é¢˜ç»Ÿè®¡')
            
            # å›¾åƒç»Ÿè®¡
            ax3 = axes[1, 0]
            img_metrics = [
                report['image_summary']['avg_mean'],
                report['image_summary']['avg_std'],
                report['image_summary']['avg_contrast']
            ]
            ax3.bar(['å¹³å‡å€¼', 'æ ‡å‡†å·®', 'å¯¹æ¯”åº¦'], img_metrics)
            ax3.set_ylabel('æ•°å€¼')
            ax3.set_title('å›¾åƒè´¨é‡æŒ‡æ ‡')
            
            # æ–‡æœ¬ç»Ÿè®¡
            ax4 = axes[1, 1]
            text_metrics = [
                report['text_summary']['avg_length'],
                report['pair_summary']['avg_correlation'] * 20  # æ”¾å¤§æ˜¾ç¤º
            ]
            ax4.bar(['å¹³å‡é•¿åº¦', 'ç›¸å…³æ€§Ã—20'], text_metrics)
            ax4.set_ylabel('æ•°å€¼')
            ax4.set_title('æ–‡æœ¬è´¨é‡æŒ‡æ ‡')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            viz_file = self.output_dir / "quality_visualization.png"
            plt.savefig(viz_file, dpi=150, bbox_inches='tight')
            plt.close()
            
            logger.info(f"è´¨é‡å¯è§†åŒ–å·²ä¿å­˜: {viz_file}")
            return str(viz_file)
            
        except Exception as e:
            logger.error(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            return "" 